{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZLRMFl0JyyQ"
   },
   "source": [
    "# **1. Perkenalan Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hssSDn-5n3HR"
   },
   "source": [
    "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
    "\n",
    "1. **Sumber Dataset**:  \n",
    "   https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification \n",
    "\n",
    "2. **Gambaran Umum**:  \n",
    "   Dataset ini merupakan dataset Predictive Maintenance (Pemeliharaan Prediktif) yang berisi data sensor dan informasi operasional dari mesin industri. Dataset ini dirancang untuk memprediksi kegagalan mesin dan mengidentifikasi jenis kegagalan yang mungkin terjadi. \n",
    "3. **Karakteristik Dataset**:\n",
    "\n",
    "   Jumlah Data\n",
    "      50.000 baris data pengamatan\n",
    "      10 kolom variabel/fitur\n",
    "\n",
    "   Variabel dalam Dataset\n",
    "      1. UDI (Unique Device Identifier): ID unik untuk setiap pengamatan (1-50000)\n",
    "\n",
    "      2. Product ID: Identifikasi produk dengan format kombinasi huruf (L/M/H) dan angka\n",
    "         - L: Low quality (Kualitas rendah)\n",
    "         - M: Medium quality (Kualitas sedang)\n",
    "         - H: High quality (Kualitas tinggi)\n",
    "      \n",
    "      3. Type: Kategori kualitas produk (L, M, atau H)\n",
    "\n",
    "      4. Air temperature [K]: Suhu udara dalam Kelvin (berkisar 295-305K)\n",
    "\n",
    "      5. Process temperature [K]: Suhu proses dalam Kelvin (berkisar 305-314K)\n",
    "\n",
    "      6. Rotational speed [rpm]: Kecepatan rotasi mesin dalam RPM (1168-2886 rpm)\n",
    "\n",
    "      7. Torque [Nm]: Torsi mesin dalam Newton-meter (3.8-76.6 Nm)\n",
    "\n",
    "      8. Tool wear [min]: Tingkat keausan alat dalam menit (0-253 menit)\n",
    "\n",
    "      9. Target: Variabel target biner (0: tidak gagal, 1: gagal)\n",
    "\n",
    "      10. Failure Type: Jenis kegagalan spesifik, termasuk:\n",
    "            - No Failure (Tidak ada kegagalan)\n",
    "            - Heat Dissipation Failure (Kegagalan disipasi panas)\n",
    "            - Power Failure (Kegagalan daya)\n",
    "            - Overstrain Failure (Kegagalan karena beban berlebih)\n",
    "            - Tool Wear Failure (Kegagalan karena keausan alat)\n",
    "            - Random Failures (Kegagalan acak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKADPWcFKlj3"
   },
   "source": [
    "# **2. Import Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgA3ERnVn84N"
   },
   "source": [
    "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlmvjLY9M4Yj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3YIEnAFKrKL"
   },
   "source": [
    "# **3. Memuat Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey3ItwTen_7E"
   },
   "source": [
    "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
    "\n",
    "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
    "\n",
    "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHCGNTyrM5fS"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\Eksperimen_SML_Zayga\\predictive_maintenance.raw')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgZkbJLpK9UR"
   },
   "source": [
    "# **4. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
    "\n",
    "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKeejtvxM6X1"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat korelasi antar fitur\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# Visualisasi korelasi dengan heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Heatmap Korelasi Antar Fitur')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(12, 8), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "df.select_dtypes(include=np.number).boxplot()\n",
    "plt.title('Boxplot untuk Deteksi Outlier pada Fitur Numerik')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpgHfgnSK3ip"
   },
   "source": [
    "# **5. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COf8KUPXLg5r"
   },
   "source": [
    "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
    "\n",
    "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
    "\n",
    "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
    "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
    "2. Menghapus Data Duplikat\n",
    "3. Normalisasi atau Standarisasi Fitur\n",
    "4. Deteksi dan Penanganan Outlier\n",
    "5. Encoding Data Kategorikal\n",
    "6. Binning (Pengelompokan Data)\n",
    "\n",
    "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1. Cek Kolom Kategorikal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek kolom kategorikal\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"Kolom kategorikal:\", categorical_cols.tolist())\n",
    "print(\"\\nDistribusi nilai pada kolom kategorikal:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2. Encoding Data Kategorikal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Membuat copy dataframe untuk preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encoding kolom Type menggunakan Label Encoding\n",
    "if 'Type' in df_processed.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_processed['Type'] = le.fit_transform(df_processed['Type'])\n",
    "    print(\"Encoding Type:\")\n",
    "    print(f\"Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Hapus kolom yang tidak diperlukan (jika ada kolom ID atau Product ID)\n",
    "columns_to_drop = ['UDI', 'Product ID'] if 'Product ID' in df_processed.columns else ['UDI']\n",
    "if all(col in df_processed.columns for col in columns_to_drop):\n",
    "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "    print(f\"\\nKolom yang dihapus: {columns_to_drop}\")\n",
    "\n",
    "print(\"\\nDataset setelah encoding:\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3. Handling Outlier (Metode IQR)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mendeteksi dan menangani outlier menggunakan IQR\n",
    "def handle_outliers_iqr(df, columns, method='cap'):\n",
    "    \"\"\"\n",
    "    Menangani outlier menggunakan metode IQR\n",
    "    method: 'cap' untuk capping, 'remove' untuk menghapus outlier\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Hitung jumlah outlier\n",
    "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n",
    "        outlier_info[col] = len(outliers)\n",
    "        \n",
    "        if method == 'cap':\n",
    "            # Capping: ganti outlier dengan batas atas/bawah\n",
    "            df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        elif method == 'remove':\n",
    "            # Hapus baris yang mengandung outlier\n",
    "            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "    \n",
    "    return df_clean, outlier_info\n",
    "\n",
    "# Pilih kolom numerik untuk handling outlier (kecuali Target)\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Target' in numeric_cols:\n",
    "    numeric_cols.remove('Target')\n",
    "\n",
    "print(f\"Jumlah data sebelum handling outlier: {len(df_processed)}\")\n",
    "\n",
    "# Handling outlier dengan metode capping\n",
    "df_processed, outlier_counts = handle_outliers_iqr(df_processed, numeric_cols, method='cap')\n",
    "\n",
    "print(f\"Jumlah data setelah handling outlier: {len(df_processed)}\")\n",
    "print(\"\\nJumlah outlier per kolom:\")\n",
    "for col, count in outlier_counts.items():\n",
    "    print(f\"{col}: {count} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4. Standarisasi Fitur Numerik**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan fitur (X) dan target (y)\n",
    "X = df_processed.drop('Target', axis=1)\n",
    "y = df_processed['Target']\n",
    "\n",
    "print(f\"Shape fitur (X): {X.shape}\")\n",
    "print(f\"Shape target (y): {y.shape}\")\n",
    "print(f\"\\nDistribusi kelas target:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nPersentase kelas:\")\n",
    "print(y.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil preprocessing ke file CSV\n",
    "df_processed.to_csv('D:\\\\Eksperimen_SML_Zayga\\\\preprocessing\\\\predictive_maintenance_processed.csv', index=False)\n",
    "print(\"Hasil preprocessing berhasil disimpan ke predictive_maintenance_processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sml (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
